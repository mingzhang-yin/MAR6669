{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import lda\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "# !pip install lda\n",
    "# !pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file  = '/content/drive/MyDrive/MAR6669-data/select_reviews.json'\n",
    "select_reviews = pd.read_json(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = select_reviews['text']\n",
    "txt = list(review_df)\n",
    "rating = list(select_reviews['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = stopwords.words('english')\n",
    "english_stopwords = set(english_stopwords)\n",
    "english_stopwords.update([\"food\", 'one'])\n",
    "txt_clean = []\n",
    "for i in range(len(txt)):    \n",
    "    txt[i] = re.sub(r'\\d+', '', txt[i]) # remove numbers\n",
    "    txt[i] = txt[i].lower()     # lower case\n",
    "    txt[i] = txt[i].translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "    txt[i] = txt[i].strip() # remove white space\n",
    "    tokens = word_tokenize(txt[i]) # tokenize\n",
    "    remove_stop = [lemmatizer.lemmatize(i) for i in tokens if not i in english_stopwords] # remove stop words and lemmatize\n",
    "    txt_clean.append(' '.join(remove_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the texts\n",
    "bag_of_words = vectorizer.fit_transform(txt_clean)\n",
    "\n",
    "# Convert to array for easy viewing\n",
    "bag_of_words_array = bag_of_words.toarray()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 5273)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 500\n",
      "INFO:lda:vocab_size: 5273\n",
      "INFO:lda:n_words: 25752\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:<0> log likelihood: -328978\n",
      "INFO:lda:<10> log likelihood: -233389\n",
      "INFO:lda:<20> log likelihood: -228689\n",
      "INFO:lda:<30> log likelihood: -226283\n",
      "INFO:lda:<40> log likelihood: -224927\n",
      "INFO:lda:<50> log likelihood: -223979\n",
      "INFO:lda:<60> log likelihood: -223355\n",
      "INFO:lda:<70> log likelihood: -222981\n",
      "INFO:lda:<80> log likelihood: -222585\n",
      "INFO:lda:<90> log likelihood: -222185\n",
      "INFO:lda:<100> log likelihood: -221762\n",
      "INFO:lda:<110> log likelihood: -221762\n",
      "INFO:lda:<120> log likelihood: -221313\n",
      "INFO:lda:<130> log likelihood: -221358\n",
      "INFO:lda:<140> log likelihood: -221346\n",
      "INFO:lda:<150> log likelihood: -221100\n",
      "INFO:lda:<160> log likelihood: -220862\n",
      "INFO:lda:<170> log likelihood: -221027\n",
      "INFO:lda:<180> log likelihood: -220915\n",
      "INFO:lda:<190> log likelihood: -220778\n",
      "INFO:lda:<200> log likelihood: -220803\n",
      "INFO:lda:<210> log likelihood: -220388\n",
      "INFO:lda:<220> log likelihood: -220386\n",
      "INFO:lda:<230> log likelihood: -220219\n",
      "INFO:lda:<240> log likelihood: -219899\n",
      "INFO:lda:<250> log likelihood: -219825\n",
      "INFO:lda:<260> log likelihood: -219913\n",
      "INFO:lda:<270> log likelihood: -219869\n",
      "INFO:lda:<280> log likelihood: -220059\n",
      "INFO:lda:<290> log likelihood: -219954\n",
      "INFO:lda:<300> log likelihood: -219893\n",
      "INFO:lda:<310> log likelihood: -219994\n",
      "INFO:lda:<320> log likelihood: -219925\n",
      "INFO:lda:<330> log likelihood: -219706\n",
      "INFO:lda:<340> log likelihood: -219649\n",
      "INFO:lda:<350> log likelihood: -219440\n",
      "INFO:lda:<360> log likelihood: -219320\n",
      "INFO:lda:<370> log likelihood: -219757\n",
      "INFO:lda:<380> log likelihood: -219250\n",
      "INFO:lda:<390> log likelihood: -219560\n",
      "INFO:lda:<400> log likelihood: -219442\n",
      "INFO:lda:<410> log likelihood: -219407\n",
      "INFO:lda:<420> log likelihood: -219314\n",
      "INFO:lda:<430> log likelihood: -219442\n",
      "INFO:lda:<440> log likelihood: -219371\n",
      "INFO:lda:<450> log likelihood: -219439\n",
      "INFO:lda:<460> log likelihood: -219495\n",
      "INFO:lda:<470> log likelihood: -219336\n",
      "INFO:lda:<480> log likelihood: -219336\n",
      "INFO:lda:<490> log likelihood: -219608\n",
      "INFO:lda:<499> log likelihood: -219403\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = lda.LDA(n_topics=20, n_iter=500, random_state=1)\n",
    "model.fit(bag_of_words_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: place best service love fish favorite definitely restaurant\n",
      "Topic 1: great hair cut kid highly dog thank see\n",
      "Topic 2: coffee first place make wanted want made im\n",
      "Topic 3: car store owner new star service better employee\n",
      "Topic 4: good nice got sandwich really breakfast enjoyed well\n",
      "Topic 5: time back great go place really good taco\n",
      "Topic 6: tree people guy car mechanic hill tell station\n",
      "Topic 7: time staff visit experience month year office issue\n",
      "Topic 8: place good nice eat menu wont better pizza\n",
      "Topic 9: even back around will said going getting though\n",
      "Topic 10: say never cant got know review take name\n",
      "Topic 11: place best great make perfect staff will burger\n",
      "Topic 12: great bar beer drink friendly night good service\n",
      "Topic 13: room party front told find stay problem guest\n",
      "Topic 14: salad thought im wing staff well side although\n",
      "Topic 15: dont pizza think little lot place really two\n",
      "Topic 16: called day phone thing now already service delivery\n",
      "Topic 17: time order minute table didnt ordered place said\n",
      "Topic 18: steak well fresh delicious meat definitely awesome cream\n",
      "Topic 19: sauce chicken good ive appetizer area dish hot\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_ \n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0 (top topic: 0)\n",
      "Doc 1 (top topic: 19)\n",
      "Doc 2 (top topic: 18)\n",
      "Doc 3 (top topic: 1)\n",
      "Doc 4 (top topic: 19)\n",
      "Doc 5 (top topic: 5)\n",
      "Doc 6 (top topic: 19)\n",
      "Doc 7 (top topic: 5)\n",
      "Doc 8 (top topic: 10)\n",
      "Doc 9 (top topic: 7)\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "for i in range(10):\n",
    "    print(\"Doc {} (top topic: {})\".format(i, doc_topic[i].argmax()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
